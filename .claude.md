# Claude Code Assistant Guide

## üéØ Project Overview

This is the **CA Fire Pipeline** - a Firecrawl-based data pipeline for processing California legal codes from [leginfo.legislature.ca.gov](https://leginfo.legislature.ca.gov).

**Production Status**: 8 codes live on https://www.codecond.com with 41,592 sections (8/30 codes complete)

## üöÄ How to Process a California Code

### Production Processing (GCloud)

**Complete Guide**: See [PRODUCTION_PROCESSING_GUIDE.md](docs/technical/PRODUCTION_PROCESSING_GUIDE.md)

**Quick Command**:
```bash
# SSH to production
gcloud compute ssh codecond --zone=us-west2-a

# Process any code (one command!)
sudo docker exec ca-fire-pipeline python scripts/process_code_complete.py <CODE>
```

**Example Codes**: CIV, CORP, PEN, PROB, FAM, EVID, CCP, GOV, etc.

### Pipeline Stages

The script automatically runs through:

1. **Stage 1 - Architecture Discovery** (~2-10 min)
   - Crawls table of contents
   - Builds hierarchical tree structure
   - Discovers all section URLs

2. **Stage 2 - Concurrent Content Extraction** (~10-40 min)
   - Extracts content using Firecrawl API
   - 15 concurrent workers (~3-5 sections/second)
   - Auto-saves progress with checkpoints

3. **Stage 3 - Multi-Version Extraction** (~0-5 min)
   - Extracts historical versions using Playwright
   - Only for sections with multiple operative dates

4. **Reconciliation** (automatic)
   - Auto-retries failed sections
   - Ensures 100% completion

### Expected Processing Times

| Code | Sections | Time | Notes |
|------|----------|------|-------|
| EVID | ~500 | 3-5 min | Small code |
| FAM, CORP | 1,500-2,500 | 15-25 min | Medium codes |
| CCP, CIV | 3,000-4,000 | 25-40 min | Large codes |
| PEN | 5,000-6,000 | 35-50 min | Very large |
| GOV | 21,000+ | 2-3 hours | Massive code |
| BPC | 10,000+ | ‚ö†Ô∏è HIGH CREDIT USAGE | Requires full API credits |

### ‚ö†Ô∏è Important: Firecrawl API Credits

**Critical**: Monitor API credits at [firecrawl.dev](https://firecrawl.dev)

- Each section = 1 API call
- Large codes (5,000+ sections) consume significant credits
- **BPC failed at 50%** due to credit exhaustion (5,055 sections missing)
- Check credit balance before processing large codes

### Monitoring Progress

**Check Database**:
```bash
sudo docker exec ca-fire-pipeline python -c "
from pipeline.core.database import DatabaseManager
from pipeline.core.config import get_settings
db = DatabaseManager(get_settings().mongodb_uri)
db.connect()

# Check specific code
count = db.section_contents.count_documents({'code': '<CODE>'})
print(f'Sections: {count:,}')

# Check all codes
codes = db.code_architectures.find({}, {'code': 1, 'total_sections': 1})
for c in codes:
    print(f\"{c['code']}: {c.get('total_sections', 0):,} sections\")
"
```

**Follow Logs**:
```bash
sudo docker exec ca-fire-pipeline tail -f /app/logs/<code>_complete_*.log
```

### Data Integrity Check

After processing, verify completeness:

```bash
# Check for incomplete sections (has_content=None means failed)
sudo docker exec ca-fire-pipeline python -c "
from pipeline.core.database import DatabaseManager
from pipeline.core.config import get_settings
db = DatabaseManager(get_settings().mongodb_uri)
db.connect()

total = db.section_contents.count_documents({'code': '<CODE>'})
complete = db.section_contents.count_documents({'code': '<CODE>', 'has_content': True})
incomplete = db.section_contents.count_documents({'code': '<CODE>', 'has_content': None})

print(f'Total: {total:,}')
print(f'Complete: {complete:,} ({complete/total*100:.1f}%)')
print(f'Incomplete: {incomplete:,}')

if incomplete > 0:
    print('‚ö†Ô∏è Code is INCOMPLETE - some sections failed extraction')
else:
    print('‚úÖ Code is COMPLETE')
"
```

## üìã Current Production Status

**Codes Live** (8/30):
- CCP: 3,354 sections
- CIV: 3,889 sections ‚úÖ (processed this session)
- CORP: 2,429 sections ‚úÖ (processed this session)
- EVID: 506 sections
- FAM: 1,626 sections
- GOV: 21,418 sections
- PEN: 5,660 sections
- PROB: 2,710 sections

**Total**: 41,592 sections across 8 codes (26.7% complete)

**BPC Status**: Removed from production (incomplete - only 50% processed due to API credit exhaustion)

## üîó Key Documentation

### Essential Guides
- **[PRODUCTION_PROCESSING_GUIDE.md](docs/technical/PRODUCTION_PROCESSING_GUIDE.md)** - Complete processing workflow
- **[PRODUCTION_STATUS.md](docs/reports/PRODUCTION_STATUS.md)** - Current production metrics
- **[GCLOUD_REPROCESS_GUIDE.md](docs/technical/GCLOUD_REPROCESS_GUIDE.md)** - Reprocessing existing codes

### Technical Documentation
- **[ARCHITECTURE_EXTRACTION_METHOD.md](docs/technical/ARCHITECTURE_EXTRACTION_METHOD.md)** - How Stage 1 works
- **[CONCURRENT_SCRAPING.md](docs/technical/CONCURRENT_SCRAPING.md)** - Stage 2 concurrent processing
- **[RECONCILIATION_SYSTEM.md](docs/technical/RECONCILIATION_SYSTEM.md)** - Auto-retry mechanism
- **[DATABASE_SCHEMA.md](docs/technical/DATABASE_SCHEMA.md)** - MongoDB schema

### Project Status
- **[README.md](README.md)** - Main project documentation
- **[PROJECT_STATUS.md](docs/technical/PROJECT_STATUS.md)** - Development roadmap

## üõ†Ô∏è Common Tasks

### Remove a Code from Production

```bash
sudo docker exec ca-fire-pipeline python -c "
from pipeline.core.database import DatabaseManager
from pipeline.core.config import get_settings
db = DatabaseManager(get_settings().mongodb_uri)
db.connect()

# Delete sections
sections_deleted = db.section_contents.delete_many({'code': '<CODE>'})
print(f'Deleted {sections_deleted.deleted_count:,} sections')

# Delete architecture
arch_deleted = db.code_architectures.delete_many({'code': '<CODE>'})
print(f'Deleted {arch_deleted.deleted_count} architecture document(s)')
"
```

### Check GCloud Environment

```bash
# SSH to production
gcloud compute ssh codecond --zone=us-west2-a

# Check Docker container
sudo docker ps | grep ca-fire-pipeline

# Check MongoDB
sudo docker exec ca-fire-pipeline python -c "
from pipeline.core.database import DatabaseManager
from pipeline.core.config import get_settings
db = DatabaseManager(get_settings().mongodb_uri)
db.connect()
print('‚úÖ MongoDB connected')
"
```

### View Processing Logs

```bash
# List all logs
sudo docker exec ca-fire-pipeline ls -lh /app/logs/

# View specific log
sudo docker exec ca-fire-pipeline cat /app/logs/<code>_complete_*.log

# Follow live log
sudo docker exec ca-fire-pipeline tail -f /app/logs/<code>_complete_*.log
```

## üêõ Known Issues

### BPC Processing Failure
- **Issue**: BPC processing failed at 50% completion (5,233/10,288 sections)
- **Cause**: Firecrawl API credits exhausted mid-processing
- **Status**: Removed from production
- **Solution**: Replenish API credits before reprocessing

### Multi-Version Section Handling
- Some sections have multiple operative dates (e.g., different versions for different dates)
- Stage 3 uses Playwright to extract all versions
- These sections are flagged with `is_multi_version=True`

### Reconciliation Limitations
- Reconciliation only retries sections with `has_content=False`
- Sections with `has_content=None` (failed extraction) are NOT automatically retried
- This is a bug in the reconciliation query logic

## üí° Best Practices

1. **Always check API credits** before processing large codes (5,000+ sections)
2. **Monitor logs in real-time** during processing
3. **Verify data integrity** after processing completes
4. **Check production status** before starting new codes
5. **Document any issues** encountered during processing

## üîí Environment

**Production**:
- **Instance**: codecond (GCloud us-west2-a)
- **Container**: ca-fire-pipeline
- **Database**: MongoDB (ca-codes-mongodb container)
- **API**: https://www.codecond.com

**Credentials**: Stored in `.env` file on GCloud instance
