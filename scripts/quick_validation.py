"""Quick Phase 1 validation - tests core functionality without full code crawl."""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from pipeline.core.database import DatabaseManager
from pipeline.services.architecture_crawler import ArchitectureCrawler
from pipeline.services.content_extractor import ContentExtractor
from pipeline.services.firecrawl_service import FirecrawlService
from pipeline.models.code import CodeCreate
from pipeline.models.section import SectionCreate

print("="*80)
print("QUICK PHASE 1 VALIDATION")
print("="*80)

# Test 1: Database Connection
print("\n‚úÖ TEST 1: Database Connection")
db = DatabaseManager()
db.connect()
print(f"   Connected to MongoDB")
print(f"   Collections: {db.db.list_collection_names()}")

# Test 2: Architecture URL (no full crawl)
print("\n‚úÖ TEST 2: Architecture Crawler (URL only)")
firecrawl = FirecrawlService()
crawler = ArchitectureCrawler(firecrawl_service=firecrawl, db_manager=db)
url = crawler.get_architecture_url("EVID")
print(f"   Architecture URL: {url}")

# Test 3: Fetch architecture page (verify text links)
print("\n‚úÖ TEST 3: Fetch Architecture Page")
result = firecrawl.scrape_url(url)
links = result["data"].get("linksOnPage", [])
text_links = [l for l in links if "codes_displayText" in l]
print(f"   Total links: {len(links)}")
print(f"   Text page links: {len(text_links)}")
if text_links:
    print(f"   Sample: {text_links[0][:80]}...")

# Test 4: Create a test section directly
print("\n‚úÖ TEST 4: Database CRUD Operations")
test_section = SectionCreate(
    code="TEST",
    section="100",
    url="https://test.com",
    content="Test content"
)
db.sections.delete_many({"code": "TEST"})
created = db.create_section(test_section)
print(f"   Created section: TEST ¬ß100")

retrieved = db.get_section("TEST", "100")
print(f"   Retrieved section: {retrieved.code} ¬ß{retrieved.section}")
print(f"   Content length: {len(retrieved.content)} chars")

# Test 5: Content Extraction (single section from YAML test data)
print("\n‚úÖ TEST 5: Content Extraction")
test_url = "https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?sectionNum=1&lawCode=FAM"
result = firecrawl.scrape_url(test_url)
markdown = result["data"].get("markdown", "")
print(f"   Scraped FAM ¬ß1")
print(f"   Markdown length: {len(markdown)} chars")
if markdown:
    # Extract content preview
    lines = markdown.split("\n")
    content_lines = [l for l in lines if l.strip() and not l.startswith("#") and not l.startswith("[")][:3]
    print(f"   Preview: {' '.join(content_lines)[:100]}...")

# Cleanup
db.sections.delete_many({"code": "TEST"})
db.disconnect()

print("\n" + "="*80)
print("‚úÖ ALL VALIDATION TESTS PASSED")
print("="*80)
print("\nüìù Summary:")
print("   1. Database connection: ‚úÖ Working")
print("   2. Architecture crawler: ‚úÖ URL generation correct")
print("   3. Text link discovery: ‚úÖ Firecrawl finds links")
print("   4. Database CRUD: ‚úÖ Create/Read working")
print("   5. Content extraction: ‚úÖ Firecrawl scrapes sections")
print("\nüöÄ Phase 1 core components are functional!")
print("üí° For full test, use a smaller code like EVID or limit max_sections")
